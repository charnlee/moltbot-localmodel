# vLLM Local æ’ä»¶ - 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹ ğŸš€

## å¿«é€Ÿé…ç½®ï¼ˆ3 æ­¥ï¼‰

### 1ï¸âƒ£ å®‰è£…æ’ä»¶

```bash
# ä» NPM å®‰è£…ï¼ˆæ¨èï¼‰
moltbot plugins install @charnlee/vllm-local

# æˆ–ä»æºç å®‰è£…
git clone https://github.com/charnlee/moltbot-localmodel.git
cd moltbot-localmodel
npm install
npm run build
moltbot plugins install .
```

### 2ï¸âƒ£ é…ç½®æ¨¡å‹

```bash
moltbot models auth login --provider vllm-local
```

é€‰æ‹© **"Manual Configuration"**ï¼Œç³»ç»Ÿä¼šæç¤ºä½ è¾“å…¥ï¼š

```
? Enter your vLLM server base URL (e.g., http://localhost:8000)
  http://localhost:8000

? Enter the model name (as deployed in vLLM)
  Qwen2.5-7B-Instruct

? Enter API key (optional, press Enter to skip)
  [ç›´æ¥å›è½¦è·³è¿‡]

? Enter context window size (tokens)
  32768

? Enter max output tokens
  4096

? Does this model support vision (images)?
  No
```

é…ç½®å®Œæˆåï¼Œä½ ä¼šçœ‹åˆ°ï¼š

```
âœ“ Configuration saved:
  - Base URL: http://localhost:8000/v1
  - Model: Qwen2.5-7B-Instruct
  - Context window: 32768 tokens
  - Max tokens: 4096
  - Vision: disabled
```

### 3ï¸âƒ£ å¼€å§‹ä½¿ç”¨

```bash
# æµ‹è¯•æ¨¡å‹
moltbot agent --model vllm-local/Qwen2.5-7B-Instruct --message "ä½ å¥½ï¼Œä½ æ˜¯å“ªä¸ªæ¨¡å‹ï¼Ÿ"

# è®¾ä¸ºé»˜è®¤æ¨¡å‹
moltbot config set agents.defaults.model vllm-local/Qwen2.5-7B-Instruct

# ç„¶åå°±å¯ä»¥ç›´æ¥ç”¨äº†
moltbot agent --message "ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
```

## éªŒè¯å®‰è£…

```bash
# æŸ¥çœ‹å·²å®‰è£…çš„æ’ä»¶
moltbot plugins list | grep vllm

# æŸ¥çœ‹å¯ç”¨çš„æ¨¡å‹
moltbot models list | grep vllm-local
```

## é€šè¿‡æ¶ˆæ¯æ¸ é“ä½¿ç”¨

é…ç½®å®Œæˆåï¼Œåœ¨ä»»ä½•æ¶ˆæ¯é€šé“ï¼ˆTelegramã€Discordã€Slack ç­‰ï¼‰ä¸­å‘é€æ¶ˆæ¯ï¼ŒAI å°±ä¼šä½¿ç”¨ä½ é…ç½®çš„ vLLM æ¨¡å‹å›å¤ï¼

```
# åœ¨ Telegram ä¸­
ä½ å¥½ï¼Œä½ æ˜¯å“ªä¸ªæ¨¡å‹ï¼Ÿ

# åˆ‡æ¢æ¨¡å‹
!model vllm-local/Qwen2.5-7B-Instruct

# ç»§ç»­å¯¹è¯
ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±
```

## é…ç½®å¤šä¸ªæ¨¡å‹

æƒ³æ·»åŠ æ›´å¤šæ¨¡å‹ï¼Ÿå†æ¬¡è¿è¡Œé…ç½®å‘½ä»¤å³å¯ï¼š

```bash
moltbot models auth login --provider vllm-local
```

æˆ–è€…æ‰‹åŠ¨ç¼–è¾‘é…ç½®æ–‡ä»¶ `~/.clawdbot/config.json5`ï¼š

```json5
{
  models: {
    providers: {
      "vllm-local": {
        baseUrl: "http://localhost:8000/v1",
        api: "openai-completions",
        models: [
          {
            id: "Qwen2.5-7B-Instruct",
            name: "Qwen2.5 7B Instruct",
            // ...é…ç½®...
          },
          {
            id: "Qwen2.5-14B-Instruct",
            name: "Qwen2.5 14B Instruct",
            // ...é…ç½®...
          }
        ]
      }
    }
  }
}
```

## ä½¿ç”¨ç¯å¢ƒå˜é‡

å¦‚æœä½ æ›´å–œæ¬¢ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼š

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export VLLM_BASE_URL="http://localhost:8000"
export VLLM_API_KEY="your-api-key"  # å¯é€‰

# è¿è¡Œé…ç½®å‘½ä»¤
moltbot models auth login --provider vllm-local
# é€‰æ‹© "Environment Variables"
```

## æµ‹è¯• vLLM æœåŠ¡å™¨

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ç›´æ¥æµ‹è¯•ä½ çš„ vLLM æœåŠ¡å™¨ï¼š

```bash
curl --request POST \
  --url http://localhost:8000/v1/chat/completions \
  --header 'Content-Type: application/json' \
  --data '{
    "model": "Qwen2.5-7B-Instruct",
    "messages": [
      {"role": "user", "content": "ä½ å¥½"}
    ]
  }'
```

## å¯åŠ¨ vLLM æœåŠ¡å™¨

å¦‚æœä½ è¿˜æ²¡æœ‰å¯åŠ¨ vLLM æœåŠ¡å™¨ï¼š

```bash
# åŸºç¡€å¯åŠ¨
python -m vllm.entrypoints.openai.api_server \
  --model /path/to/Qwen2.5-7B-Instruct \
  --served-model-name Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000

# ä¼˜åŒ–é…ç½®
python -m vllm.entrypoints.openai.api_server \
  --model /path/to/Qwen2.5-7B-Instruct \
  --served-model-name Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --gpu-memory-utilization 0.9 \
  --max-model-len 32768
```

## å®Œæˆï¼ğŸ‰

ç°åœ¨ä½ å¯ä»¥ç”¨æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹äº†ï¼

æ›´å¤šé«˜çº§åŠŸèƒ½å’Œæ•…éšœæ’é™¤ï¼Œè¯·å‚è€ƒ [å®Œæ•´æ–‡æ¡£](./README.md)ã€‚
